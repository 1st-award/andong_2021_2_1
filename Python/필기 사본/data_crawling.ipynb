{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "data_crawling.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMZWlaV0Fvge86P9REGvj29",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/1st-award/andong_2021_2_1/blob/main/Python/%ED%95%84%EA%B8%B0%20%EC%82%AC%EB%B3%B8/data_crawling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QMUXeRhq9st"
      },
      "source": [
        "# 웹 크롤링(Web Crawling)과 **웹 스크래핑(Web Scraping)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZO4zmeyWrEss"
      },
      "source": [
        "## 웹 크롤링\n",
        "- 웹 크롤러(Web Crawler)\n",
        "  - 조직적, 자동화된 방법으로 월드 와이드 웹을 탐색하는 컴퓨터 프로그램, 자동화 봇(bot\n",
        "  - Google, Bing 등과 같은 검색 엔진은 데이터를 최신 상태로 유지하기 위하여 여러 사이트에서 웹 크롤링\n",
        "- 웹 크롤링(Web Crawling)\n",
        "  - 자동화 봇인 웹 크롤러가 웹에서 링크를 타고다니며 웹 페이지들을 수집하는 행위\n",
        "  - 주로 웹 상의 기존 페이지를 다운로드하여 복사본 생성\n",
        "  - 엡 스파이더링이라고도 함\n",
        "  - 예: Google, Yahoo, Bing 등의 봇이 전 세계의 웹 사이트를 수집하여 검색 서비스를 제공하는 행위"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPslReNbrl4v"
      },
      "source": [
        "### **웹페이지 크롤링 주의사항**\n",
        "1. 스크랩하는 콘텐츠에 지적재산권이 있는지 확인한다.\n",
        "2. 짧은 시간에 과도하게 데이터를 수집하여 해당 사이트에 부담을 주지 않는지 확인한다.\n",
        "3. 해당 사이트의 이용방침을 위반하지 않는지 확인한다.\n",
        "4. 사용자의 민감한 정보를 가져오지 않는지 확인한다.\n",
        "5. 가져온 콘텐츠를 적합하게 사용해야한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7v_90TtEsB1K"
      },
      "source": [
        "### 스크래핑 과정 요약\n",
        "1. 스크래핑할 데이터 정의\n",
        "2. 웹 페이지 선정\n",
        "3. 선정한 웹 페이지의 HTML 코드에서 추출하고자 하는 데이터 분석\n",
        "4. 원하는 웹 페이지에 **request를 보내어 html 정보를 받음**\n",
        "5. **받은 html 정보를 파싱**\n",
        "6. **필요한 정보 추출**\n",
        "7. 추출한 정보 저장"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhkF_6i_soEM"
      },
      "source": [
        "### 웹 페이지 파싱\n",
        "파싱(Parsing)은 **구문 분석**이라고 하며 문장을 이루고 있는 구성 요소를 분해하고 분해된 각 요소의 위계 관계를 분석하여 구조를 결정하는 것을 말한다. 즉 가공되지 않은 데이터를 분해하여 원하는 형태로 조립하고 다시 정보를 빼내는 작을 말한다. **웹 페이지 파싱**은 **웹**에서 내가 원하는 데이터를 특정 패턴이나 순서로 추출하여 정보로 가공하는 것이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AQr9iWLtBoR"
      },
      "source": [
        "## 순서 : 웹 크롤링 -> 웹 스크래핑 -> 파싱"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehdsJPRatIdd"
      },
      "source": [
        "### requests와 Beautiful Soup을 이용하여 웹 스크래핑하기\n",
        "- requests : 웹 페이지의 HTML 문서에 담긴 내용을 가져 오도록 request(요청)하는 HTTP 라이브러리\n",
        "- Beautiful Soup: requests 결과로 반환받은 (복잡한) HTML 문서를 탐색해서 원하는 추출할 수 있도록 분석해주는 파이썬 라이브러리.\n",
        "- Beautiful Soup:은 날 것의 html을 의미있는 객체로 만들어서 사용자가 요리하기 쉽게 만들어 준다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yF-txFy0Au_P"
      },
      "source": [
        "## request.get(url)함수 이용하여 원하는 URL에 데이터 요청하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgPwiVpGA0yY"
      },
      "source": [
        "### **당근마켓 중고거래 인기매물(https://www.daangn.com/hot_articles)** 페이지 데이터 읽어오기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5iQPrDhq9RN"
      },
      "source": [
        "import requests as rq\n",
        "# 웹 페이지 가져오기\n",
        "daagnReq = rq.get(\"https://www.daangn.com/hot_articles\")\n",
        "# 사람이 읽기 쉬운 텍스트 형태의 코드\n",
        "print(daagnReq.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfSS_XRqBiDC"
      },
      "source": [
        "### **국립안동대학교(https://www.andong.ac.kr/main/index.do)** 메인 페이지의 html 소스 읽기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjLSVXjlBd5X"
      },
      "source": [
        "import requests as rp\n",
        "# 웹 페이지 가져오기\n",
        "anuReq = rq.get(\"https://www.andong.ac.kr/main/index.do\")\n",
        "# 컴퓨터가 처리하기 쉬운 바이너리 코드 형식\n",
        "print(anuReq.content) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXw8wpLtB5I6"
      },
      "source": [
        "### 네이버 영화랭킹 페이지의 html 소스 읽기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hY7-fslOB8Bc"
      },
      "source": [
        "import requests\n",
        "\n",
        "requests = requests.get(\"https://movie.naver.com/movie/sdb/rank/rmovie.nhn\")\n",
        "html = requests.text\n",
        "print(html)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MF6mRn3VCgSt"
      },
      "source": [
        "## 요청한 웹 페이지의 텍스트 데이터에서 원하는 데이터 추출하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5zeljrbCktg"
      },
      "source": [
        "# 안동대학교 웹 페이지 파싱하기\n",
        "from bs4 import BeautifulSoup as bs\n",
        "anuSoup = bs(anuReq.content, 'html.parser')\n",
        "print(anuSoup)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jy4iTK3oCyUs"
      },
      "source": [
        "# 당근마켓 중고거래 인기매물 페이지 파싱하기\n",
        "from bs4 import BeautifulSoup as bs\n",
        "daggnSoup = bs(daagnReq.content, 'html.parser')\n",
        "print(daggnSoup)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPN4rLJqDgx7"
      },
      "source": [
        "### **파싱한 문서에서 필요한 데이터 추출하기**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-d8E9PNDk4p"
      },
      "source": [
        "# 필요한 데이터(안동대학교 홈페이지의 제목) 추출하기\n",
        "mydata = anuSoup.find('title')\n",
        "print(mydata)\n",
        "# 추출한 데이터 활용하기\n",
        "for string in mydata:\n",
        "  print(string)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txveEbvXEmrm"
      },
      "source": [
        "## 태그 속성을 이용하여 데이터를 크롤링하는 방법\n",
        "\n",
        "- **find: 첫 번째 태그를 리턴**\n",
        "  - data = soup.find('p', class_ = 'class name')\n",
        "  - data = soup.find('p', 'class name')\n",
        "  - data = soup.find('p', attrs = {'align':'center'})\n",
        "  - data = soup.find(id = 'name')\n",
        "\n",
        "- **findAll: 조건에 해당되는 모든 태그를 리스트로 리턴**\n",
        "  - data = soup.findAll()\n",
        "- **select_one(): 조건에 맞는 태그가 여러 개 있어도 한 개만 가져옴**\n",
        "  - data = soup.select_one('태크이름')\n",
        "  - data = soup.select_one('.클래스이름')\n",
        "  - data = soup.select_one('#아이디이름')\n",
        "  - data = soup.select_one('상위태크이름>자식태그>자식태그')\n",
        "  - data = soup.select_one('태그이름.클래스이름')\n",
        "  - data = soup.select_one('#아이디이름>태그이름.클래스이름')\n",
        "\n",
        "- **select(): 조건에 맞는 태그를 여러 개 가져옴**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bX2Sr8pqF-tj"
      },
      "source": [
        "### **안동대학교 홈페이지 상단 배너 정보 읽기**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzAgUE9vFNVs"
      },
      "source": [
        "import requests as rq\n",
        "from bs4 import BeautifulSoup as bs\n",
        "\n",
        "anuRes = rq.get('https://www.andong.ac.kr/main/index.do')\n",
        "soup = bs(anuRes.content, 'html.parser')\n",
        "mydata = soup.find(id=\"top-banner\")\n",
        "print(mydata)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJ5hd1NQG8q6"
      },
      "source": [
        "import requests as rq\n",
        "from bs4 import BeautifulSoup as bs\n",
        "\n",
        "anuRes = rq.get('https://www.andong.ac.kr/main/index.do')\n",
        "soup = bs(anuRes.content, 'html.parser')\n",
        "top_banner = soup.findAll('p', 'tit')\n",
        "for t in top_banner:\n",
        "  print(t.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LElqKhDpHvhQ"
      },
      "source": [
        "import requests as rq\n",
        "from bs4 import BeautifulSoup as bs\n",
        "\n",
        "anuRes = rq.get('https://www.andong.ac.kr/main/index.do')\n",
        "soup = bs(anuRes.content, 'html.parser')\n",
        "top_banner = soup.select('#popupList > div > div > p.tit')\n",
        "for t in top_banner:\n",
        "  print(t.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myzadF-9HvH3"
      },
      "source": [
        "### **당근마켓 중고거래 인기매물 페이지의 정보 읽기**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwHbPNjiISn-"
      },
      "source": [
        "import requests as rq\n",
        "from bs4 import BeautifulSoup as bs\n",
        "\n",
        "res = rq.get('https://www.daangn.com/hot_articles')\n",
        "soup = bs(res.content, 'html.parser')\n",
        "# 판매중인 물건 이름 추출\n",
        "sale = soup.findAll('h2', 'card-title')\n",
        "for productName in sale:\n",
        "  # print(productName.text)\n",
        "  print(productName.text.strip())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrVc3v0SI6ae"
      },
      "source": [
        "res = rq.get('https://www.daangn.com/hot_articles')\n",
        "soup = bs(res.content, 'html.parser')\n",
        "# 판매중인 물건 이름 추출\n",
        "sale = soup.findAll('div', 'card-price ')\n",
        "for productPrice in sale:\n",
        "  # print(productName.text)\n",
        "  print(productPrice.text.strip())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkGRUh70Jcsk"
      },
      "source": [
        "import requests as rq\n",
        "from bs4 import BeautifulSoup as bs\n",
        "\n",
        "res = rq.get('https://www.daangn.com/hot_articles')\n",
        "soup = bs(res.content, 'html.parser')\n",
        "\n",
        "# 판매중인 물건의 이름, 가격, 판매지역 추출\n",
        "info_1 = soup.findAll('h2', 'card-title')\n",
        "info_2 = soup.findAll('div', 'card-price ')\n",
        "info_3 = soup.findAll('div', 'card-region-name')\n",
        "\n",
        "prodName = []\n",
        "prodPrice = []\n",
        "address = []\n",
        "\n",
        "for name in info_1:\n",
        "  prodName.append(name.text.strip())\n",
        "for price in info_2:\n",
        "  prodPrice.append(price.text.strip())\n",
        "for addr in info_3:\n",
        "  address.append(addr.text.strip())\n",
        "\n",
        "address"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpNPg22cKK6_"
      },
      "source": [
        "### **11번가 '디지털/가전' 카테코리의 '도전! 베스트' 상품 리스트 스크랩핑**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uc6-brbeJ_aK"
      },
      "source": [
        "from bs4 import BeautifulSoup as bs\n",
        "import requests as rq\n",
        "\n",
        "res = rq.get('https://www.11st.co.kr/browsing/BestSeller.tmall?method=getBestSellerMain&cornerNo=10')\n",
        "soup = bs(res.content, 'html.parser')\n",
        "dataList = soup.select('#challengeBestAdList > li > div > a > div.pname > p')\n",
        "for data in dataList:\n",
        "  print(data.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1f-EBSlYLnVQ"
      },
      "source": [
        "### **11번가 '디지털/가전' 카테고리의 '베스트500' 상품 리스트 크롤링**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jttPamnbLtzb"
      },
      "source": [
        "res = rq.get('https://www.11st.co.kr/browsing/BestSeller.tmall?method=getBestSellerMain&cornerNo=10')\n",
        "soup = bs(res.content, 'html.parser')\n",
        "dataList = soup.select('#bestPrdList > div > ul > li > div > a > div.pname > p')\n",
        "for data in dataList:\n",
        "  print(data.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCJWIHM_Mg5i"
      },
      "source": [
        "### **네이버 영화 >> 영화랭킹**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImH62g8TM1Jb"
      },
      "source": [
        "import requests as rq\n",
        "from bs4 import BeautifulSoup as builder_registry\n",
        "\n",
        "response = rq.get('https://movie.naver.com/movie/sdb/rank/rmovie.nhn')\n",
        "html = response.text\n",
        "soup = bs(html, 'html.parser')\n",
        "movieRank = soup.select('#old_content > table > tbody > tr > td.title > div > a')\n",
        "for movie in movieRank:\n",
        "  print(movie.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WM8OleP5M0uw"
      },
      "source": [
        "### **네이버 영화 >> 상영 예정작**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNmnqsNdNrsX"
      },
      "source": [
        "import requests as rq\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup as builder_registry\n",
        "\n",
        "res = rq.get('https://movie.naver.com/movie/running/current.nhn')\n",
        "soup = bs(res.content, 'html.parser')\n",
        "movieTitle = soup.select('#content > div.article > div > div.lst_wrap > ul > li > dl > dt > a')\n",
        "movieRating = soup.select('#content > div.article > div > div.lst_wrap > ul > li > dl > dd.star > dl.info_star > dd > div > a > span.num')\n",
        "movieVote = soup.select('#content > div.article > div > div.lst_wrap > ul > li > dl > dd.star > dl.info_star > dd > div > a > span.num2 > em')\n",
        "\n",
        "movie = []\n",
        "rating = []\n",
        "participants = []\n",
        "for title in movieTitle:\n",
        "  movie.append(title.text.strip())\n",
        "for rate in movieRating:\n",
        "  rating.append(rate.text.strip())\n",
        "for vote in movieVote:\n",
        "  participants.append(vote.text.strip())\n",
        "\n",
        "print(len(movie))\n",
        "print(len(rating))\n",
        "print(len(participants))\n",
        "naverMovie = pd.DataFrame({'제목' : movie, '평점' : rating, '참여인원' : participants})\n",
        "naverMovie.to_csv(\"/gdrive/My Drive/Python/naverMovie.csv\", index=False, encoding='utf-8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0d5PpbWRM35"
      },
      "source": [
        "### **네이버 금융의 인기검색종목**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bif0IAFQQjZL"
      },
      "source": [
        "import requests as rq\n",
        "from bs4 import BeautifulSoup as builder_registry\n",
        "\n",
        "res = rq.get('https://finance.naver.com/')\n",
        "soup = bs(res.content, 'html.parser')\n",
        "dataList = soup.select(\"#container > div.aside > div.group_aside > div.aside_area.aside_popular > table > tbody > tr\")\n",
        "# dataList = soup.select(\"#container > div.aside > div.group_aside > div.aside_area.aside_popular > table > tbody > tr > th > a\")\n",
        "\n",
        "for data in dataList:\n",
        "  item = data.find('a').get_text()\n",
        "  price = data.find('td').get_text()\n",
        "  diff = data.find('span').get_text()\n",
        "  print(item, price, diff)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1piqxDPmSkIE"
      },
      "source": [
        "## 3_웹크롤링_뉴스 헤드라인 검색"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNwtJTSMS44s"
      },
      "source": [
        "import requests as rq\n",
        "from bs4 import BeautifulSoup as builder_registry\n",
        "\n",
        "res = rq.get('https://news.daum.net/digital#1')\n",
        "soup = bs(res.content, 'html.parser')\n",
        "headList = soup.select(\"#cSub > div > div.section_cate.section_headline > ul.list_mainnews > li > div.cont_thumb > strong > a\")\n",
        "\n",
        "for data in headList:\n",
        "  print(data.get_text())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jcy5NfrLT-sy"
      },
      "source": [
        "import requests as rq\n",
        "from bs4 import BeautifulSoup as builder_registry\n",
        "\n",
        "res = rq.get('https://news.daum.net/')\n",
        "soup = bs(res.content, 'html.parser')\n",
        "headLineNews1 = soup.select(\"#cSub > div > ul > li > div.item_issue > div > strong > a\")\n",
        "headLineNews2 = soup.select(\"#mArticle > div.box_headline > ul > li > strong > a\")\n",
        "\n",
        "for head1 in headLineNews1:\n",
        "  print(head1.get_text())\n",
        "for head2 in headLineNews2:\n",
        "  print(head2.get_text())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwFM2JCPbNLj"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYSyXf6SbgfF"
      },
      "source": [
        "import requests as rq\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup as bs\n",
        "\n",
        "res = rq.get('https://news.daum.net/')\n",
        "soup = bs(res.content, 'html.parser')\n",
        "headLineNews1 = soup.select(\"#cSub > div > ul > li > div.item_issue > div > strong > a\")\n",
        "journal1 = soup.select('#cSub > div > ul > li > div.item_issue > div > span')\n",
        "headLineNews2 = soup.select(\"#mArticle > div.box_headline > ul > li > strong > a\")\n",
        "journal2 = soup.select(\"#mArticle > div.box_headline > ul > li > strong > span\")\n",
        "\n",
        "title = []\n",
        "press = []\n",
        "for head1 in headLineNews1:\n",
        "  title.append(head1.get_text())\n",
        "for j1 in journal1:\n",
        "  press.append(j1.get_text())\n",
        "for head2 in headLineNews2:\n",
        "  title.append(head2.get_text())\n",
        "for j2 in journal2:\n",
        "  press.append(j2.get_text())\n",
        "\n",
        "daumNewsHome = pd.DataFrame({\"헤드라인\": title, \"언론사\": press})\n",
        "daumNewsHome.to_excel('/gdrive/My Drive/Python/daumNewsHome.xlsx', sheet_name='daumNews')\n",
        "daumNewsHome"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}